---
title: "Scraping Tweets about Green Hydrogen in Mobility"
author: "Xiying LIU, HSLU, xiying.liu@stud.hslu.ch"
date: "15.09.2021"
output:
  pdf_document:
---

##Setup
For some APIs there is already an R package available that facilitates the access. In the case of Twitter there are many, we use the package `rtweet` for this example (see https://rtweet.info/).

```{r eval=FALSE, warn.conflicts=F, quietly=T, message=F}
source('myfunctions.r')
```
In order to be able to access the API, we need to create a Twitter app here: 

https://developer.twitter.com/en/apps

We can then find our API keys under "Details > Keys and Tokens" and create an access token using the `create_token` function from `rtweet`.

```{r eval=FALSE}
create_token(
  app = "HydrogenMobility",
  consumer_key = "S2ZekeyafDX1UiPVASkbcydD9",
  consumer_secret = "Rppk0P13iVyw58e7PvLb2Eh8mb2RwLlnDW2lsXV14EvUTvy8AY",
  access_token = "957197191394856961-CYyVWpBD9AxbKiNEI8dKxMFxkTyF65F",
  access_secret = "bLvvlJHiUMB2hOATPSGJGcHJ3gG1jLqtWEqa17uuGQMIv")

```

##The Twitter API and rtweet
Now we are ready to use the API. The Twitter API is quite sophisticated with a lot of options. One can search tweets, for instance, which is described here:
https://developer.twitter.com/en/docs/tweets/search/overview

The `rtweet` reference can be found here:
https://rtweet.info/reference/index.html

We now run a simple call to search for tweets and then display the results in an insightful way (example inspired by rtweet.info).

First, we search for 18,000 tweets that contain the #greenhydrogen hashtag. Twitter rate limits cap the number of search results returned to 18,000 every 15 minutes (see details on rate limits here: https://developer.twitter.com/en/docs/basics/rate-limits.html). To request more than that, simply set retryonratelimit = TRUE and rtweet will wait for rate limit resets for you.

```{r eval=FALSE}
datascience_tweets <- search_tweets(q = "datascience", 
                    include_rts = FALSE, retryonratelimit = TRUE)
```


```{r}
nrow(datascience_tweets)
```
###########################################
############# Analyze Tweets ##############
###########################################


** Most frequently used words
Now lets create a frequency chart of the hashtags used together with greenhydrogen

```{r eval=FALSE}
htags <- data.frame()
htemp <- list()
maxrows <- nrow(hydrogen_tweets)

for(i in 1:maxrows){
  txt <- hydrogen_tweets$text[i]
  
  #use a regular expression to match all hashtags
  htemp <- str_extract_all(txt, "#\\S+", TRUE)
  #\\S match non-whitespace characters, at least one

  #iterate over all hashtags and store them into our table
  if(ncol(htemp) != 0){
    for(j in 1:ncol(htemp)){
      htags[i,j] <- htemp[1,j]
    }
  }
}
```

Change from data.frame to list (`unlist`), then, count number of occurrences using `table`.
```{r eval=FALSE}
htags_list <- unlist(htags)
htags_list_withcounts <- table(htags_list)

tagcounts <- as.data.frame(htags_list_withcounts)
tagcounts_sorted <- tagcounts[order(-tagcounts$Freq),]
```

Lets see the most frequent hashtags and store them into a CSV file.

```{r eval=FALSE}
head(tagcounts_sorted)

barplot(tagcounts_sorted[1:20,]$Freq, las = 2, 
        names.arg = tagcounts_sorted[1:20,]$htags_list, 
        cex.names = 0.8,
        col ="deepskyblue3", border="white",
        main ="Most frequent #words",
        ylab = "Frequency",
        ylim = range(pretty(c(0, tagcounts_sorted[1:20,]$Freq))))
```

```{r}
#write to csv
write.csv(tagcounts_sorted, "ht_sort_HASHTAG2.csv", fileEncoding = "UTF-8")
```

```{r}
head(tagcounts_sorted)
```

```{r}
barplot(tagcounts_sorted[1:25,]$Freq, las = 1, horiz=T, 
        names.arg = tagcounts_sorted[1:25,]$htags_list, cex.names = 0.8,
        col ="#69b3a2", main ="Most frequent words",
        ylab = "Word frequencies")
```


```{r}
#write to csv
write.csv(tagcounts_sorted, "ht_sort_HASHTAG2.csv", fileEncoding = "UTF-8")

```

For fun, let's display a wordcloud based on the information about the hashtags and their frequencies. There is a convenient package available named `wordcloud` for this purpose.

```{r eval=FALSE}
#install.packages('wordcloud')
library('wordcloud')

wordcloud(tagcounts_sorted[0:200,1], tagcounts_sorted[0:200,2], 
          max.words=100,
          random.order = FALSE, random.color = FALSE, 
          colors = "cornflowerblue")
```


##Tweets & Locations
Example with geocodes.

```{r eval=FALSE}

## search for 10,000 tweets sent from the US
hydrogen_tweets_usa <- search_tweets(
  "#greenhydrogen lang:en", geocode = lookup_coords("usa"), n = 500
)

#backup tweets
geocode_usa <- hydrogen_tweets_usa
```

```{r eval=FALSE}
## create lat/lng variables using all available tweet and profile geo-location data
hydrogen_tweets_usa <- lat_lng(hydrogen_tweets_usa)
```

```{r}
## plot state boundaries
#install.packages("maps")
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)

## plot lat and lng points onto state map
with(hydrogen_tweets_usa, points(lng, lat, pch = 21, cex = 1.75, col = rgb(0, .9, .7, .75)))

# hydrogen_tweets$lng

```



##Exercises
1. Try out different API endpoints, have a look at the rtweet reference for this purpose (https://rtweet.info/reference/index.html).
2. *Run a sentiment analysis of the above tweets using the code from before.

## Sentiment analysis

```{r}
hydrogen_sent = hydrogen_tweets %>% select(screen_name, text)

